---
title: "Regression Analysis"
author: "Stephan.Huber@hs-fresenius.de"
date: 'Hochschule Fresenius: Data Science'
output:
  html_document:
    toc: yes
    toc_depth: 2
    number_sections: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
  pdf_document:
    toc: yes
    toc_depth: '3'
    number_sections: yes
---

```{r setup, include=FALSE}
library("knitr")
knit_hooks$set(purl = hook_purl)
```

---

Compiled at `r format(Sys.time(), '%d %B, %Y')`

Word count: `r as.integer(sub("(\\d+).+$", "\\1", system(sprintf("wc -w %s", knitr::current_input()), intern = TRUE))) - 20`

---

# Literature

Regression analysis is covered by almost all econometric and statistical textbooks. Some are more formal some are more illustrative and intuitive. Here is my selection with a focus on R:

- [Book: Applied Statistics with R](https://daviddalpiaz.github.io/appliedstats/)
- [Book: Introduction to Econometrics with R](https://www.econometrics-with-r.org/)
- A more formal approach from my Alma Mater: [slides](https://www.uni-regensburg.de/wirtschaftswissenschaften/vwl-tschernig/medien/zeitreihenoekonometrie/eoe_eng_2020_08_chapter_all.pdf) and [handout](https://www.uni-regensburg.de/wirtschaftswissenschaften/vwl-tschernig/medien/methoden-der-oekonometrie/methoden_oekonometrie_handout_2018_11_29.pdf)
- This presentation is available on [my github account](https://github.com/hubchev/courses) 


# Why regression analysis?

## It is a medicine agains alternative facts

![](/home/sthu/Dropbox/hsf/pic/destat/afact2.png)

- Regressions allow us to **draw insights** from data, 
- to analyze and **interpret** the strength of relationships and 
- to reduce the likeliness of **causal fallacy**.


## Simpsons paradox

![](/home/sthu/Dropbox/hsf/pic/destat/foo-13.png)

---

![](/home/sthu/Dropbox/hsf/pic/destat/foo-32.png)


## Correlation does not imply causation

![](/home/sthu/Dropbox/hsf/pic/destat/storks.png){ width=75% }

---

![](/home/sthu/Dropbox/hsf/pic/destat/nejm.jpg){ width=80% }
[http://www.nejm.org/doi/full/10.1056/NEJMon1211064](http://www.nejm.org/doi/full/10.1056/NEJMon1211064)

_Sometimes called spurious correlation*_

### Correlation is often useless

- Correlation reflects the \textit{direction} of a linear relationship (top row), it tells us nothing about the slope (strength) of that relationship (middle). Thus, we cannot interpret them economically.
- Moreover, many aspects of nonlinear relationships (bottom) remain unexplained. The figure in the center has a slope of 0 but in that case the correlation coefficient is undefined because the variance of $Y$ is zero.

![](/home/sthu/Dropbox/hsf/pic/destat/corelate_overview.png){ width=65% }

---

- **Linear regression** is a predictive modeling techniques that aims to find a mathematical equation for a variable $y$ as a function of one (simple linear model) or more variables (multiple linear regression), $x$. 

- The method to \textit{fit a line} is called the **ordinary least squared (OLS) method** as it minimizes the sum of the squared differences of all $y_i$ and $y_i^*$ as sketched below.

![](/home/sthu/Dropbox/hsf/pic/destat/regression_ols.png){ width=50% }


---

The simple linear regression model is
\[
y_i = \beta_{0} + \beta_{1} x_i + \epsilon_i
\]
where

-  the index $i$ runs over the observations, $i=1,\dots,n$
-  $y_i$ is the \textbf{dependent variable}, the regressand
-  $x_i$ is the \textbf{independent variable}, the regressor
-  $\beta_0$ is the \textbf{intercept} or constant
-  $\beta_1$ is the slope of regression line
-  $\epsilon_i$ is the \textbf{error term} or the residual.

# OLS estimation method

- minimize the squared residuals by choosing the estimated coefficients $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$ 

\[\min_{\hat{\beta_{0}}, \hat{\beta_{1}}}\sum_{i=1} \epsilon_i^2  =  \sum_{i=1}	(y_i - \hat{\beta_{0}} - \hat{\beta_{1}} x_i)^2\]

-  Minimizing the function requires to calculate the first order conditions with respect to $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$  and set them zero (see exercises)

- The estimators are:
\[\hat{\beta_{0}} =\bar{y}-\hat{\beta_{1}}\bar{x}\]

\[\hat{\beta_{1}} =\frac{\sum_{i=1}(y_i -\bar{y})(x_i-\bar{x})}{\sum_{i=1} (\bar{x} -  x_i)^2 }\]

---

In the multiple regression model the OLS is derived similarly but we skip the derivation. The $\hat{\beta}$ coefficient vector can be expressed as follows:
\[\hat{\beta}=\left(\mathrm{X} ^{\mathsf {T}}\mathrm {X} \right)^{-1}\mathrm {X} ^{\mathsf {T}}\mathbf {y}\]
![](/home/sthu/Dropbox/hsf/pic/destat/intuitive_1.png){ width=20% } ![](/home/sthu/Dropbox/hsf/pic/destat/intuitive_2.png){ width=30% }

Here you find the derivation of the OLS estimator for the multiple regression model: [ch. 9.1 Applied Statistics with R](https://daviddalpiaz.github.io/appliedstats/multiple-linear-regression.html)



# Caveats of OLS (outliers are bad)

On this [website](https://www.econometrics-with-r.org/4-2-estimating-the-coefficients-of-the-linear-regression-model.html) you find an interactive application. Play around with it and discuss possible caveats of the OLS method.

![](/home/sthu/Dropbox/hsf/pic/destat/reg-outlier.png){ width=85% }

# Example 

In the statistic course of WS 2020, I asked 23 students about their weight, height, sex, and number of siblings:

```{r, echo = TRUE}
library("haven")
classdata <- read.csv("https://raw.githubusercontent.com/hubchev/courses/main/dta/classdata.csv")

head(classdata)
```

---

```{r, echo = TRUE}

summary(classdata)
```


## How to execute a regression analysis

1. get known to the data 
2. build a theory on how the variables may be related
3. derive a estimated equation from your theory
4. estimate
5. evaluate your empirics
6. go back to 2. and improve your theory
7. interpret your results

A more elaborated flow diagram for regression analysis can be found below. <sub><sup><sub>
Source: <http://medrescon.tripod.com/regression_explained.pdf>
</sup></sub></sup>

---

![](/home/sthu/Dropbox/hsf/pic/destat/olsscheme.png){ width=70% }

## First look at data


```{r pressure, echo=TRUE}
library("ggplot2")
ggplot(classdata, aes(x=height, y=weight)) + geom_point() 

```

---

include a regression line:

```{r , echo=TRUE}
ggplot(classdata, aes(x=height, y=weight)) +
  geom_point() +
  stat_smooth(formula=y~x, method="lm", se=FALSE, colour="red", linetype=1)

```

---

distinguish male/female by including a seperate constant:

```{r , echo=TRUE}
## baseline regression  model
model  <- lm(weight ~ height + sex , data = classdata )
show(model)
interm <- model$coefficients[1] 
slope  <- model$coefficients[2]
interw <- model$coefficients[1]+model$coefficients[3] 
```

---

```{r, echo=TRUE}
summary(model)
```

---

```{r, echo=TRUE}
ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point() +
  geom_abline(slope = slope, intercept = interw, linetype = 2, size=1.5)+
  geom_abline(slope = slope, intercept = interm, linetype = 2, size=1.5) +
  geom_abline(slope = coef(model)[[2]], intercept = coef(model)[[1]]) 

```

does not look to good, maybe we should introduce also different slopes for m/w

---

```{r , echo=TRUE}

ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point( aes(size = 2)) +
  stat_smooth(formula = y ~ x,  method = "lm", 
              se = FALSE, colour = "red", linetype = 1)

```


---

Can we use other available variables: siblings?

```{r , echo=TRUE}
ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point( aes(size = siblings)) 

```

---

```{r , echo=TRUE}
## baseline model
model  <- lm(weight ~ height + sex , data = classdata )

ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point( aes(size = 2)) +
  stat_smooth(formula = y ~ x,  
              method = "lm", 
              se = T, 
              colour = "red", 
              linetype = 1)

```


---

Let us look at regression output:

```{r, echo=TRUE, results='hide'}

m1 <- lm(weight ~ height , data = classdata )
m2 <- lm(weight ~ height + sex , data = classdata )
m3 <- lm(weight ~ height + sex + height * sex , data = classdata )
m4 <- lm(weight ~ height + sex + height * sex + siblings , data = classdata )
m5 <- lm(weight ~ height + sex + height * sex , data = subset(classdata, siblings < 4 ))

library(sjPlot)
tab_model(m1, m2, m3, m4, m5,
          p.style = "stars",
          p.threshold = c(0.2, 0.1, 0.05),
          show.ci = FALSE, 
          show.se = FALSE) 
```

---

```{r, echo=FALSE}
tab_model(m1, m2, m3, m4, 
          p.style = "stars",
          p.threshold = c(0.2, 0.1, 0.05),
          show.ci = FALSE, 
          show.se = FALSE) 
```

---

excluding outliers with four siblings:

```{r, echo=FALSE}
tab_model(m3, m5,
          p.style = "stars",
          p.threshold = c(0.2, 0.1, 0.05),
          show.ci = FALSE, 
          show.se = FALSE) 
```

## Interpretation of the results

- We can make predictions about the impact of height on male and female
- As both, the intercept and the slope differs for male and female we should interpret the regressions seperately:
- One centimeter more for **MEN** is *on average* and *ceteris paribus* related with 0.16 kg more weight.
- One centimeter more for **WOMEN** is *on average* and *ceteris paribus* related with 1.01 kg more weight.

## Regression Diagnostics

Linear Regression makes several assumptions about the data, the model assumes that:

- The relationship between the predictor (x) and the dependent variable (y) has linear relationship.
- The residuals are assumed to have a constant variance.
- The residual errors are assumed to be normally distributed.
- Error terms are independent and have zero mean.

More on regression Diagnostics can be found [Applied Statistics with R: 13 Model Diagnostics](https://daviddalpiaz.github.io/appliedstats/model-diagnostics.html#r-markdown-6)

---

```{r, echo=T}
plot(residuals(m3), fitted(m3))
plot(residuals(m3), classdata$siblings)
```

## Measures of fit

### R squared 

![](/home/sthu/Dropbox/hsf/pic/destat/howbad.png){ width=80% }

---

$R^2$ is the fraction of the sample variance of $Y_i$ that is explained by $X_i$.  It can be written as the ratio of the explained sum of squares (ESS) to the total sum of squares (TSS):
\[ESS  =  \sum_{i = 1}^n \left( \hat{Y_i} - \bar{Y} \right)^2,\]
\[TSS  =  \sum_{i = 1}^n \left( Y_i - \bar{Y} \right)^2,\]
\[R^2  = \frac{ESS}{TSS}.\]

---

Since $TSS = ESS + SSR$ we can also write 

$R^2 = 1- \frac{SSR}{TSS}$ 

with $SSR=\sum_{i=1} \epsilon_i^2$

![](/home/sthu/Dropbox/hsf/pic/destat/fitR.png){ width=55% }


$R^2$ lies between 0 and 1. It is easy to see that a perfect fit, i.e., no errors made when fitting the regression line, implies $R2=1$ since then we have $SSR=0$. On the contrary, if our estimated regression line does not explain any variation in the $Y_i$, we have $ESS=0$ and consequently $R^2=0$.

### Adjusted R-squared

Including more independent variables into an estimated model must decrease SSR. Thus, the R-squared never decreases, when adding new variables even when it just a chance correlation between variables. Having more coefficients to estimate the precision at which we can estimate the effects decrease. Thus, we need to deal with the trade-off. The adjusted $R^2$ can help here:  

\[{\displaystyle {\bar {R}}^{2}=1-(1-R^{2}){n-1 \over n-p-1}}\]

Always use adjusted $R^2$ when you compare specifications with different number of coefficients.
  


## The miracle of CONTROL VARIABLES in multiple regressions

Control variables are usually variables that you are not particularly interested in, but that are related to the dependent variable. You want to remove their effects from the equation. A control variable enters a regression in the same way as an independent variable -- the method is the same.

 ![](/home/sthu/Dropbox/hsf/pic/destat/storks3.png){ width=25% }
 

## When do we need (more) control variables

From the [Gauss-Markov theorem](https://www.econometrics-with-r.org/5-5-the-gauss-markov-theorem.html) we know that if the [OLS assumptions](https://www.econometrics-with-r.org/6-4-ols-assumptions-in-multiple-regression.html) are fullfiled, the OLS estimator is (in the sense of smallest variance) the **best linear conditionally unbiased estimator (BLUE)**.

However, OLS estimates can suffer from **omitted variable bias** when the regressor, X, is correlated with an omitted variable. For omitted variable bias to occur, two conditions must be fulfilled:

    1. X is correlated with the omitted variable.
    2. The omitted variable is a determinant of the dependent variable Y.

---

![](/home/sthu/Dropbox/hsf/pic/destat/storks2.png){ width=80% }



# Take away messages

- Regressions rule the world and may kill alternative facts.
- Correlation does not imply causation.
- It is hard to find the true *data generating process*.


![](/home/sthu/Dropbox/hsf/pic/destat/correlation_causation.png){ width=80% }





```{r eval=FALSE}
rmarkdown::render("regress_lecture.Rmd", "all")
```

```{bash, engine.opts='-l'}
wkhtmltopdf regress_lecture.html regress_lecture.pdf
```


